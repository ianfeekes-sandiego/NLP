{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Assignment: Ian Feekes NLP Basics Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all content for USD MSAAI NLP Fundamentals Class for Ian Feekes' Module 4 Assignment. Thank you for taking the time to grade my work and to help me grow with your feedback.\n",
    "\n",
    "If any work here does not end up in the correct location in blackboard, or does not meet standards or expectations, please let me know and I will gratefully and expediently make corrections. (ifeekes@sandiego.edu, 916-333-9381)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import pandas and Read in quora_questions.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is composed and ran locally, so the quora_questions.csv file will be imported from the same directory in which this .ipynb file resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# hard-coded path for the csv file\n",
    "dataFilePath = './quora_questions.csv'\n",
    "\n",
    "# read the csv file into data frame structure df\n",
    "df = pd.read_csv(dataFilePath)\n",
    "\n",
    "# break if something went wrong with loading\n",
    "assert(df.shape[0] > 0 and df.shape[1] > 0)\n",
    "\n",
    "# spit out the first 5 entries of the data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Preprocessing: Use TF-IDF Vectorization to create a vectorized document term matrix. You may want to explore the max_df and min_df parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<404289x38972 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4002064 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import TF-IDF Vectorizer using sci-kit learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore the terms that appear in more than 50% of the documents\n",
    "maxDf = 0.95\n",
    "# Ignore the terms that appear in less than 1% of the documents\n",
    "minDf = 2\n",
    "\n",
    "# Initialize structure\n",
    "v = TfidfVectorizer(min_df = minDf, max_df = maxDf)\n",
    "x = v.fit_transform(df['Question'])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Using Scikit-Learn create an instance of NMF with 20 expected components. (Use random state=42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NMF(n_components=20, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(n_components=20, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NMF(n_components=20, random_state=42)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import scikit-learn Non-negative Matrix Factorization\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200, n_components = 20,\n",
    "                random_state=42, shuffle=False, solver='cd', tol=0.0001, verbose=0)\n",
    "nmf_model.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Print out the top 15 most common words for each of the 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepika\n",
      "zuckerberg\n",
      "dresden\n",
      "jazz\n",
      "halfhand\n",
      "papers\n",
      "rom\n",
      "fade\n",
      "arkham\n",
      "discounts\n",
      "iiser\n",
      "webflow\n",
      "dependant\n",
      "feathers\n",
      "mayhem\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(15):\n",
    "    # Note: get_feature_names is depricated, using new function instead\n",
    "    random_word_id = random.randint(0, len(v.get_feature_names_out()))\n",
    "    print(v.get_feature_names_out()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmf_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.78675197e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.05994678e-04, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 2.35053620e-05, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.11400208e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.07294173e-04, 4.74667823e-03, 3.89814157e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.94484834e-03, 7.98495003e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38972"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmf_model.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['effects', 'out', 'battle', 'time', 'rid', 'purpose', 'life', 'all', 'was', 'world', 'most', 'what', 'meaning', 'the', 'of']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['work', 'know', 'weight', 'stop', 'an', 'find', 'from', 'start', 'think', 'with', 'become', 'people', 'get', 'how', 'do']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['they', 'facts', 'people', 'most', 'movies', 'books', 'things', 'about', 'that', 'ways', 'good', 'examples', 'some', 'what', 'are']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['did', 've', 'about', 'love', 'someone', 'know', 'thing', 'that', 'when', 'think', 'would', 'ever', 'if', 'have', 'you']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['have', 'need', 'from', 'time', 'take', 'get', 'ways', 'want', 'possible', 'be', 'like', 'learn', 'way', 'it', 'to']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['2016', 'movie', 'books', 'laptop', 'language', 'programming', 'movies', 'ever', 'learn', 'book', 'under', 'way', 'which', 'the', 'best']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['that', 'than', 'hate', 'is', 'have', 'many', 'don', 'and', 'did', 'do', 'we', 'not', 'so', 'people', 'why']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['lose', 'with', 'weight', 'become', 'an', 'from', 'be', 'learn', 'one', 'find', 'we', 'where', 'get', 'how', 'can']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #8\n",
      "['meaning', 'most', 'the', 'better', 'com', 'way', 'an', 'like', 'that', 'thing', 'or', 'there', 'it', 'what', 'is']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #9\n",
      "['any', 'places', 'live', 'many', 'most', 'which', 'job', 'life', 'where', 'engineering', 'world', 'there', 'the', 'india', 'in']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #10\n",
      "['science', 'western', 'pakistan', 'relationship', 'similarities', 'chinese', 'an', 'compare', 'war', 'differences', 'what', 'the', 'difference', 'between', 'and']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #11\n",
      "['sex', 'take', 'one', 'when', 'long', 'much', 'what', 'have', 'how', 'feel', 'work', 'like', 'mean', 'it', 'does']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #12\n",
      "['learning', 'gate', 'books', 'book', 'preparation', 'new', 'engineering', 'free', 'exam', 'an', 'year', '2017', 'good', 'prepare', 'for']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #13\n",
      "['add', 'delete', 'many', 'instagram', 'people', 'there', 'asked', 'google', 'answer', 'answers', 'ask', 'question', 'questions', 'quora', 'on']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #14\n",
      "['am', 'girlfriend', 'speaking', 'phone', 'writing', 'gmail', 'increase', 'if', 'password', 'me', 'account', 'skills', 'english', 'improve', 'my']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #15\n",
      "['black', 'easiest', 'through', 'home', 'easy', 'much', 'youtube', 'how', 'ways', 'way', 'from', 'earn', 'online', 'make', 'money']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #16\n",
      "['government', 'currency', 'economy', 'india', 'ban', 'indian', 'banning', 'black', 'rupee', 'will', 'rs', 'and', '1000', 'notes', '500']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #17\n",
      "['resolutions', 'most', 'movie', 'moment', 'resolution', 'favourite', 'new', '2017', 'review', 'year', 'was', 'what', 'favorite', 'life', 'your']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #18\n",
      "['us', 'happen', 'election', 'hillary', 'or', 'president', 'clinton', 'win', 'if', 'would', 'donald', 'will', 'who', 'be', 'trump']\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #19\n",
      "['be', 'start', 'one', 'their', 'what', 'employees', 'going', 'into', 'new', 'things', 'day', 'at', 'first', 'know', 'should']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{index}\")\n",
    "    print([v.get_feature_names_out()[i] for i in topic.argsort()[-15:]], end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above cell if not all is shown should be:\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #0\n",
    "['effects', 'out', 'battle', 'time', 'rid', 'purpose', 'life', 'all', 'was', 'world', 'most', 'what', 'meaning', 'the', 'of']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #1\n",
    "['work', 'know', 'weight', 'stop', 'an', 'find', 'from', 'start', 'think', 'with', 'become', 'people', 'get', 'how', 'do']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #2\n",
    "['they', 'facts', 'people', 'most', 'movies', 'books', 'things', 'about', 'that', 'ways', 'good', 'examples', 'some', 'what', 'are']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #3\n",
    "['did', 've', 'about', 'love', 'someone', 'know', 'thing', 'that', 'when', 'think', 'would', 'ever', 'if', 'have', 'you']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #4\n",
    "['have', 'need', 'from', 'time', 'take', 'get', 'ways', 'want', 'possible', 'be', 'like', 'learn', 'way', 'it', 'to']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #5\n",
    "['2016', 'movie', 'books', 'laptop', 'language', 'programming', 'movies', 'ever', 'learn', 'book', 'under', 'way', 'which', 'the', 'best']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #6\n",
    "['that', 'than', 'hate', 'is', 'have', 'many', 'don', 'and', 'did', 'do', 'we', 'not', 'so', 'people', 'why']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #7\n",
    "['lose', 'with', 'weight', 'become', 'an', 'from', 'be', 'learn', 'one', 'find', 'we', 'where', 'get', 'how', 'can']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #8\n",
    "['meaning', 'most', 'the', 'better', 'com', 'way', 'an', 'like', 'that', 'thing', 'or', 'there', 'it', 'what', 'is']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #9\n",
    "['any', 'places', 'live', 'many', 'most', 'which', 'job', 'life', 'where', 'engineering', 'world', 'there', 'the', 'india', 'in']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #10\n",
    "['science', 'western', 'pakistan', 'relationship', 'similarities', 'chinese', 'an', 'compare', 'war', 'differences', 'what', 'the', 'difference', 'between', 'and']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #11\n",
    "['sex', 'take', 'one', 'when', 'long', 'much', 'what', 'have', 'how', 'feel', 'work', 'like', 'mean', 'it', 'does']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #12\n",
    "['learning', 'gate', 'books', 'book', 'preparation', 'new', 'engineering', 'free', 'exam', 'an', 'year', '2017', 'good', 'prepare', 'for']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #13\n",
    "['add', 'delete', 'many', 'instagram', 'people', 'there', 'asked', 'google', 'answer', 'answers', 'ask', 'question', 'questions', 'quora', 'on']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #14\n",
    "['am', 'girlfriend', 'speaking', 'phone', 'writing', 'gmail', 'increase', 'if', 'password', 'me', 'account', 'skills', 'english', 'improve', 'my']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #15\n",
    "['black', 'easiest', 'through', 'home', 'easy', 'much', 'youtube', 'how', 'ways', 'way', 'from', 'earn', 'online', 'make', 'money']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #16\n",
    "['government', 'currency', 'economy', 'india', 'ban', 'indian', 'banning', 'black', 'rupee', 'will', 'rs', 'and', '1000', 'notes', '500']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #17\n",
    "['resolutions', 'most', 'movie', 'moment', 'resolution', 'favourite', 'new', '2017', 'review', 'year', 'was', 'what', 'favorite', 'life', 'your']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #18\n",
    "['us', 'happen', 'election', 'hillary', 'or', 'president', 'clinton', 'win', 'if', 'would', 'donald', 'will', 'who', 'be', 'trump']\n",
    "\n",
    "THE TOP 15 WORDS FOR TOPIC #19\n",
    "['be', 'start', 'one', 'their', 'what', 'employees', 'going', 'into', 'new', 'things', 'day', 'at', 'first', 'know', 'should']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "assert(nlp)\n",
    "\n",
    "# This vector is numerically closer to cat than to dandelion\n",
    "lionVector = nlp(u'lion').vector\n",
    "\n",
    "# In spacy these vectors are stored as 300 item arrays\n",
    "assert(len(lionVector) == 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc and span objects themselves have vectors derived from the averages of individual token vectors\n",
    "# This makes it possible to compare entire documents\n",
    "doc = nlp(u'The quick brown fox jumped over the lazy dog.')\n",
    "\n",
    "assert(len(doc.vector) == 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.3854507803916931\n",
      "lion pet 0.20031584799289703\n",
      "cat lion 0.3854507803916931\n",
      "cat cat 1.0\n",
      "cat pet 0.732966423034668\n",
      "pet lion 0.20031584799289703\n",
      "pet cat 0.732966423034668\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a three-token doc object\n",
    "tokens = nlp(u'lion cat pet')\n",
    "\n",
    "# Iterate through token combinations\n",
    "for t1 in tokens:\n",
    "    for t2 in tokens:\n",
    "        # Built-in similarity method is great for token comparison\n",
    "        print(t1.text, t2.text, t1.similarity(t2))\n",
    "\n",
    "# Below we see that cat is more of a pet than it is a lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = nlp(u'auditor engineer superstar doctor student celebrity actor')\n",
    "\n",
    "# for t1 in tokens:\n",
    "#     for t2 in tokens:\n",
    "#         if t1 == t2: continue\n",
    "#         print(t1.text, t2.text, t1.similarity(t2))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "the\n",
      "and\n",
      "that\n",
      "where\n",
      "she\n",
      "they\n",
      "woman\n",
      "there\n",
      "should\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cosine_similarity = lambda x, y: 1- spatial.distance.cosine(x,y)\n",
    "\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "# Now to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "new_vector = king - man + woman\n",
    "\n",
    "computed_similarities = []\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words\n",
    "    if not(word.has_vector) or not(word.is_lower) or not(word.is_alpha): continue\n",
    "    # Get the similarity and add it to the list\n",
    "    similarity = cosine_similarity(new_vector, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "\n",
    "for i in computed_similarities[:10]:\n",
    "    print(i[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "npr = pd.read_csv('npr.csv')\n",
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11992x54777 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3033388 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(npr['Article'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=7, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=7, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=7, random_state=42)\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.64332806e+00, 2.38014333e+03, 1.42900522e-01, ...,\n",
       "        1.43006821e-01, 1.42902042e-01, 1.42861626e-01],\n",
       "       [2.76191749e+01, 5.36394437e+02, 1.42857148e-01, ...,\n",
       "        1.42861973e-01, 1.42857147e-01, 1.42906875e-01],\n",
       "       [7.22783888e+00, 8.24033986e+02, 1.42857148e-01, ...,\n",
       "        6.14236247e+00, 2.14061364e+00, 1.42923753e-01],\n",
       "       ...,\n",
       "       [3.11488651e+00, 3.50409655e+02, 1.42857147e-01, ...,\n",
       "        1.42859912e-01, 1.42857146e-01, 1.42866614e-01],\n",
       "       [4.61486388e+01, 5.14408600e+01, 3.14281373e+00, ...,\n",
       "        1.43107628e-01, 1.43902481e-01, 2.14271779e+00],\n",
       "       [4.93991422e-01, 4.18841042e+02, 1.42857151e-01, ...,\n",
       "        1.42857146e-01, 1.43760101e-01, 1.42866201e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert(len(LDA.components_) == 7)\n",
    "\n",
    "LDA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LDA.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2475 18302 35285 ... 22673 42561 42993]\n",
      "0.1428571430851871\n",
      "6247.245510521083\n",
      "Top 10 most representative words for this topic: \n",
      "33390\n",
      "36310\n",
      "21228\n",
      "10425\n",
      "31464\n",
      "8149\n",
      "36283\n",
      "22673\n",
      "42561\n",
      "42993\n",
      "\n",
      "Now in string flavor: \n",
      "new\n",
      "percent\n",
      "government\n",
      "company\n",
      "million\n",
      "care\n",
      "people\n",
      "health\n",
      "said\n",
      "says\n",
      "\n",
      " Least favorite: \n",
      "alliterative\n",
      "fcbd\n",
      "overstreet\n",
      "skippable\n",
      "isayama\n",
      "cbldf\n",
      "mechanika\n",
      "funnies\n",
      "jughead\n",
      "freecomicbookday\n"
     ]
    }
   ],
   "source": [
    "from numpy import single\n",
    "\n",
    "\n",
    "single_topic = LDA.components_[0]\n",
    "\n",
    "# Returns the indices that would sort this array\n",
    "print(single_topic.argsort())\n",
    "argumentIndices = single_topic.argsort()\n",
    "\n",
    "# Word that represents this topic the least\n",
    "print(single_topic[argumentIndices[0]])\n",
    "\n",
    "# Word that represents this topic the most\n",
    "print(single_topic[argumentIndices[-1]])\n",
    "\n",
    "indexList = []\n",
    "print(\"Top 10 most representative words for this topic: \")\n",
    "for i in single_topic.argsort()[-10:]:\n",
    "    indexList.append(i)\n",
    "    print(i)\n",
    "\n",
    "print(\"\\nNow in string flavor: \")\n",
    "for index in indexList:\n",
    "    print(cv.get_feature_names_out()[index])\n",
    "\n",
    "leastFavorite = [i for i in single_topic.argsort()[:10]]\n",
    "print(\"\\n Least favorite: \")\n",
    "for idx in leastFavorite:\n",
    "    print(cv.get_feature_names_out()[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
