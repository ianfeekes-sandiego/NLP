{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113d2959",
   "metadata": {},
   "source": [
    "# Module 2 Assignment: Ian Feekes NLP Basics Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbc4d7",
   "metadata": {},
   "source": [
    "This notebook contains all content for USD MSAAI NLP Fundamentals Class for Ian Feekes' Module 2 Assignment. Thank you for taking the time to grade my work and to help me grow with your feedback.\n",
    "\n",
    "If anyt work here does not end up in the correct location in blackboard, or does not meet standards or expectations, please let me know and I will gratefully and expediently make corrections. (ifeekes@sandiego.edu, 916-333-9381)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a507c28",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886d42a",
   "metadata": {},
   "source": [
    "### Perform Standard Imports\n",
    "The below cell performs standard imports as per the assignment sheet, along with any other imports used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c97b9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to perform standard imports\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63eb3f",
   "metadata": {},
   "source": [
    "### 1) Create a Doc object from the file owlcreek.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2bc55a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create doc object from local file with explicit path\n",
    "doc = nlp(open('./textfiles/owlcreek.txt').read())\n",
    "\n",
    "# Break if program didn't collect any data successfully\n",
    "assert(doc)\n",
    "assert(len(doc) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19289a9e",
   "metadata": {},
   "source": [
    "### Verify Doc Object Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b31bbe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
       "\n",
       "by Ambrose Bierce\n",
       "\n",
       "I\n",
       "\n",
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the swift water twenty feet below.  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to verify it worked: \n",
    "\n",
    "doc[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21e764",
   "metadata": {},
   "source": [
    "### 2) How many tokens are contained in the file?\n",
    "There are 4835 tokens in the owl creek text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ec5d2fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4835 tokens in the owl creek text file\n"
     ]
    }
   ],
   "source": [
    "numTokens = 0\n",
    "for token in doc:\n",
    "    numTokens = numTokens + 1\n",
    "\n",
    "assert(numTokens == len(doc))\n",
    "\n",
    "print(\"There are\", numTokens, \"tokens in the owl creek text file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3fd11",
   "metadata": {},
   "source": [
    "### 3) How many sentences are contained in the file?\n",
    "There are 201 sentences in the owl creek text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1dd566d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 201 sentences in the owl creek text file\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "numSentences = len(sentences)\n",
    "\n",
    "print(\"There are\", numSentences, \"sentences in the owl creek text file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ce073",
   "metadata": {},
   "source": [
    "### 4) Print the second sentence in the document\n",
    "The man's hands were behind\n",
    "his back, the wrists bound with a cord. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "50f12eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The man's hands were behind\n",
      "his back, the wrists bound with a cord.  \n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c23b50",
   "metadata": {},
   "source": [
    "### 5) For each token in the below sentence, print its text, POS tag, dep tag, and LEMMA. Challenge: have values line up in columns in the output\n",
    "A man stood upon a railroad bridge in northern Alabama, looking down into the swift water twenty feet below\n",
    "\n",
    "#### Note: hard-coded the sentence since spacy or the text file was combining the title w/ the first sentence. Did this in order to achieve consistent output with the expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "19b65de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DET det a\n",
      "man NOUN nsubj man\n",
      "stood VERB ROOT stand\n",
      "upon SCONJ prep upon\n",
      "a DET det a\n",
      "railroad NOUN compound railroad\n",
      "bridge NOUN pobj bridge\n",
      "in ADP prep in\n",
      "northern ADJ amod northern\n",
      "Alabama PROPN pobj Alabama\n",
      ", PUNCT punct ,\n",
      "looking VERB advcl look\n",
      "down ADV prep down\n",
      "\n",
      "  SPACE dep \n",
      " \n",
      "into ADP prep into\n",
      "the DET det the\n",
      "swift ADJ amod swift\n",
      "water NOUN pobj water\n",
      "twenty NUM nummod twenty\n",
      "feet NOUN npadvmod foot\n",
      "below ADV advmod below\n",
      ". PUNCT punct .\n",
      "\n",
      " SPACE dep \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NORMAL SOLUTION\n",
    "\n",
    "# Note: hard-coded the first sentence since either spacy or the owlcreek file was failing to split the title from the first sentence.\n",
    "currSentence = nlp(\"A man stood upon a railroad bridge in northern Alabama, looking down \\n into the swift water twenty feet below. \\n\")\n",
    "# If we wanted to do the first sentence with the title as my program does, we would uncomment the following line and run the block:\n",
    "# currSentence = sentences[0]\n",
    "\n",
    "for token in currSentence:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6758c844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A              DET    det       a\n",
      "man            NOUN   nsubj     man\n",
      "stood          VERB   ROOT      stand\n",
      "upon           SCONJ  prep      upon\n",
      "a              DET    det       a\n",
      "railroad       NOUN   compound  railroad\n",
      "bridge         NOUN   pobj      bridge\n",
      "in             ADP    prep      in\n",
      "northern       ADJ    amod      northern\n",
      "Alabama        PROPN  pobj      Alabama\n",
      ",              PUNCT  punct     ,\n",
      "looking        VERB   advcl     look\n",
      "down           ADV    prep      down\n",
      "\n",
      "              SPACE  dep       \n",
      " \n",
      "into           ADP    prep      into\n",
      "the            DET    det       the\n",
      "swift          ADJ    amod      swift\n",
      "water          NOUN   pobj      water\n",
      "twenty         NUM    nummod    twenty\n",
      "feet           NOUN   npadvmod  foot\n",
      "below          ADV    advmod    below\n",
      ".              PUNCT  punct     .\n",
      "\n",
      "              SPACE  dep       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHALLENGE SOLUTION\n",
    "for token in currSentence:\n",
    "    print(\"{0:15}{1:7}{2:10}{3}\".format(token.text, token.pos_, token.dep_, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a65e3",
   "metadata": {},
   "source": [
    "### 6) Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "35f40101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher Library\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "319f6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pattern and add it to matcher:\n",
    "pattern = [{\"LOWER\": \"swimming\"}, {\"IS_SPACE\": True}, {\"LOWER\": \"vigorously\"}]\n",
    "matcher.add('swimming',[pattern], on_match=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "aa3fd863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11901859001352538922, 1274, 1277), (11901859001352538922, 3609, 3612)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of matches called \"found_matches\" and print the list:\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "00b8586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swimming\n",
      "vigorously\n",
      "swimming\n",
      "vigorously\n"
     ]
    }
   ],
   "source": [
    "# Just to verify the textual content of the matches\n",
    "for match_id, start, end in found_matches:\n",
    "    print(doc[start: end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992c431",
   "metadata": {},
   "source": [
    "### 7) Print the text surrounding each found match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "93f2219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " By diving I could evade the bullets and, swimming\n",
      "vigorously, reach the bank, take to the woods and\n",
      "\n",
      "saw all this over his shoulder; he was now swimming\n",
      "vigorously with the current.  His brain was as energetic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simply prints the 10 tokens adjacent to either side of the match. There is likely a built-in surround method to be investigated\n",
    "for match_id, start, end in found_matches:\n",
    "    print(doc[max(0, start-10): min(len(doc), end+10)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ffe3f",
   "metadata": {},
   "source": [
    "### Extra Credit: Print the sentence that contains each match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "85c6707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By diving I could evade the bullets and, swimming\n",
      "vigorously, reach the bank, take to the woods and get away home.  \n",
      "\n",
      "The hunted man saw all this over his shoulder; he was now swimming\n",
      "vigorously with the current.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty global list of dictionaries to contain matched sentences & their attributes\n",
    "matchedSentences = []\n",
    "\n",
    "# Helper function. Creates a matched span and appends relevant data to array above\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    # Offsets span information with doc sentence information\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - span.sent.start_char,\n",
    "        \"end\": span.end_char - span.sent.start_char,\n",
    "        \"label\": \"MATCH\",\n",
    "    }]\n",
    "    # Appends to global array\n",
    "    matchedSentences.append({\"text\": span.sent.text, \"ents\": match_ents})\n",
    "\n",
    "\n",
    "# Add new label for the same pattern with our above function built-in to the match algorithm\n",
    "matcher.add('b',[pattern], on_match=collect_sents)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print the text contents of the sentence\n",
    "for sentence in matchedSentences:\n",
    "    print(sentence['text'])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
